{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from chessEngine import ChessEncoder, MLPEngine\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fen_value</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57383</th>\n",
       "      <td>rnbqk2r/ppp2ppp/4pn2/3pb3/2PP4/4P3/PP3PPP/RN1Q...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189240</th>\n",
       "      <td>rnb1k1nr/pppp1ppp/8/2b5/8/1P2P1q1/PBPP2PP/RN2K...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201335</th>\n",
       "      <td>r3k3/pp1n3p/2ppp3/4p3/7B/1P2P3/P1PP1PPP/RN2K1N...</td>\n",
       "      <td>-682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183753</th>\n",
       "      <td>1q3k2/p4p1p/1pn1r1p1/8/2N5/PP2PBP1/5PQP/3R1RK1...</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88036</th>\n",
       "      <td>rn1q2r1/pbpp1Q1p/2k1p3/N1p2p2/3P4/2P5/P4PPP/R1...</td>\n",
       "      <td>-258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93080</th>\n",
       "      <td>6k1/2p3pp/p4p2/3p1K2/1Pb2P2/r7/6PP/8 w - - 0 36</td>\n",
       "      <td>-650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110254</th>\n",
       "      <td>6k1/1R3pp1/2P3r1/q2Pp2p/3bP3/3B1P2/1r2Q1PP/5K1...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>rnbqkbnr/pp1ppppp/8/2p5/4P3/8/PPPP1PPP/RNBQKBN...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141340</th>\n",
       "      <td>rn2kbnr/pbpp1ppp/1p2pq2/8/2PP4/P1N2P2/1P2P1PP/...</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125674</th>\n",
       "      <td>r3r3/ppp2p2/3p1Q1k/3P1n2/8/5P2/PP5P/R5RK b - -...</td>\n",
       "      <td>-8498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fen_value  score\n",
       "57383   rnbqk2r/ppp2ppp/4pn2/3pb3/2PP4/4P3/PP3PPP/RN1Q...     22\n",
       "189240  rnb1k1nr/pppp1ppp/8/2b5/8/1P2P1q1/PBPP2PP/RN2K...    102\n",
       "201335  r3k3/pp1n3p/2ppp3/4p3/7B/1P2P3/P1PP1PPP/RN2K1N...   -682\n",
       "183753  1q3k2/p4p1p/1pn1r1p1/8/2N5/PP2PBP1/5PQP/3R1RK1...    785\n",
       "88036   rn1q2r1/pbpp1Q1p/2k1p3/N1p2p2/3P4/2P5/P4PPP/R1...   -258\n",
       "...                                                   ...    ...\n",
       "93080     6k1/2p3pp/p4p2/3p1K2/1Pb2P2/r7/6PP/8 w - - 0 36   -650\n",
       "110254  6k1/1R3pp1/2P3r1/q2Pp2p/3bP3/3B1P2/1r2Q1PP/5K1...    345\n",
       "3954    rnbqkbnr/pp1ppppp/8/2p5/4P3/8/PPPP1PPP/RNBQKBN...     35\n",
       "141340  rn2kbnr/pbpp1ppp/1p2pq2/8/2PP4/P1N2P2/1P2P1PP/...    -85\n",
       "125674  r3r3/ppp2p2/3p1Q1k/3P1n2/8/5P2/PP5P/R5RK b - -...  -8498\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fen_analysis.csv').sample(frac=1)[:30000] # This shuffles the rows\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_object = ChessEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fen_encodings = []\n",
    "for fen_i in df['fen_value']:\n",
    "    encoded_fen = encoder_object.encode_fen(fen_i)\n",
    "    fen_encodings.append(encoded_fen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(fen_encodings, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 11, 10,  ...,  0, 36, 39],\n",
       "        [ 9, 11, 10,  ...,  0, 29, 38],\n",
       "        [ 9,  0,  0,  ...,  0, 27, 15],\n",
       "        ...,\n",
       "        [ 9, 11, 10,  ...,  0, 39, 39],\n",
       "        [ 9, 11,  0,  ...,  0, 39, 39],\n",
       "        [ 9,  0,  0,  ...,  0, 24, 18]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encodings = []\n",
    "for y_i in df['score']:\n",
    "    # print(y_i)\n",
    "    encoded_score = encoder_object.encode_score(str(y_i))\n",
    "    score_encodings.append(encoded_score)\n",
    "y = torch.tensor(score_encodings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30000, 200]), torch.Size([30000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 10000\n",
    "test_split = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, bs):\n",
    "    \n",
    "    assert isinstance(X, torch.Tensor)\n",
    "    assert isinstance(Y, torch.Tensor)\n",
    "\n",
    "    batch = torch.randint(0, len(X), (bs,))\n",
    "    x = X[batch].to(device)\n",
    "    y = Y[batch].to(device).to(torch.bfloat16)\n",
    "    return x, y\n",
    "# b = get_batch(X, y, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[:val_split].to(device)\n",
    "y_val = y[:val_split].to(device)\n",
    "X_test = X[val_split:test_split].to(device)\n",
    "y_test = y[val_split:test_split].to(device)\n",
    "X = X[test_split:]\n",
    "y = y[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.1\n",
    "num_steps = 3000\n",
    "warmup_steps = 50\n",
    "bs = len(X)\n",
    "# allowed_error = 100 #\n",
    "d1 = {1:10, 2:20}\n",
    "if bs > len(X): bs = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPEngine(embedding_dim=64).to(device)\n",
    "loss_category = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = lr) # i accidenly used a smaller lr for scheduler and it worked better, maybe try it?\n",
    "# Define warm-up and decay\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_steps:  \n",
    "        return epoch / warmup_steps\n",
    "    else:  # Exponential decay after warm-up\n",
    "        return 0.95 ** (epoch - warmup_steps)\n",
    "\n",
    "scheduler = LambdaLR(optimiser, lr_lambda)\n",
    "model = model.to(torch.bfloat16)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = {}\n",
    "val_history = {}\n",
    "start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  3375104.0\n",
      "1 :  3555328.0\n",
      "2 :  3325952.0\n",
      "3 :  3358720.0\n",
      "4 :  3211264.0\n",
      "5 :  3211264.0\n",
      "6 :  3309568.0\n",
      "7 :  3325952.0\n",
      "8 :  3342336.0\n",
      "9 :  3588096.0\n",
      "10 :  3375104.0\n",
      "11 :  3342336.0\n",
      "12 :  3260416.0\n",
      "13 :  3489792.0\n",
      "14 :  3358720.0\n",
      "15 :  3293184.0\n",
      "16 :  3637248.0\n",
      "17 :  3538944.0\n",
      "18 :  3309568.0\n",
      "19 :  3588096.0\n",
      "20 :  3424256.0\n",
      "21 :  3325952.0\n",
      "22 :  3260416.0\n",
      "23 :  3227648.0\n",
      "24 :  3178496.0\n",
      "25 :  3080192.0\n",
      "26 :  3145728.0\n",
      "27 :  3162112.0\n",
      "28 :  2998272.0\n",
      "29 :  2801664.0\n",
      "30 :  3047424.0\n",
      "31 :  2932736.0\n",
      "32 :  2834432.0\n",
      "33 :  2785280.0\n",
      "34 :  2703360.0\n",
      "35 :  2555904.0\n",
      "36 :  2490368.0\n",
      "37 :  2392064.0\n",
      "38 :  2408448.0\n",
      "39 :  2228224.0\n",
      "40 :  2129920.0\n",
      "41 :  1884160.0\n",
      "42 :  1990656.0\n",
      "43 :  1736704.0\n",
      "44 :  1409024.0\n",
      "45 :  1376256.0\n",
      "46 :  1220608.0\n",
      "47 :  1019904.0\n",
      "48 :  954368.0\n",
      "49 :  888832.0\n",
      "50 :  786432.0\n",
      "51 :  577536.0\n",
      "52 :  489472.0\n",
      "53 :  448512.0\n",
      "54 :  317440.0\n",
      "55 :  337920.0\n",
      "56 :  258048.0\n",
      "57 :  239616.0\n",
      "58 :  206848.0\n",
      "59 :  227328.0\n",
      "60 :  192512.0\n",
      "61 :  159744.0\n",
      "62 :  179200.0\n",
      "63 :  162816.0\n",
      "64 :  150528.0\n",
      "65 :  148480.0\n",
      "66 :  133120.0\n",
      "67 :  133120.0\n",
      "68 :  125952.0\n",
      "69 :  133120.0\n",
      "70 :  110592.0\n",
      "71 :  116736.0\n",
      "72 :  113152.0\n",
      "73 :  106496.0\n",
      "74 :  105472.0\n",
      "75 :  92160.0\n",
      "76 :  110080.0\n",
      "77 :  102400.0\n",
      "78 :  89600.0\n",
      "79 :  86528.0\n",
      "80 :  89088.0\n",
      "81 :  83456.0\n",
      "82 :  82944.0\n",
      "83 :  85504.0\n",
      "84 :  77824.0\n",
      "85 :  78336.0\n",
      "86 :  78848.0\n",
      "87 :  83968.0\n",
      "88 :  82432.0\n",
      "89 :  71168.0\n",
      "90 :  79360.0\n",
      "91 :  71680.0\n",
      "92 :  74752.0\n",
      "93 :  75776.0\n",
      "94 :  65536.0\n",
      "95 :  67072.0\n",
      "96 :  70656.0\n",
      "97 :  61952.0\n",
      "98 :  69120.0\n",
      "99 :  62976.0\n",
      "100 :  69632.0\n",
      "101 :  60928.0\n",
      "102 :  65536.0\n",
      "103 :  70144.0\n",
      "104 :  65536.0\n",
      "105 :  65536.0\n",
      "106 :  66560.0\n",
      "107 :  64512.0\n",
      "108 :  60416.0\n",
      "109 :  64768.0\n",
      "110 :  67584.0\n",
      "111 :  64000.0\n",
      "112 :  72192.0\n",
      "113 :  62720.0\n",
      "114 :  59392.0\n",
      "115 :  65024.0\n",
      "116 :  62720.0\n",
      "117 :  63232.0\n",
      "118 :  66048.0\n",
      "119 :  59136.0\n",
      "120 :  66048.0\n",
      "121 :  69632.0\n",
      "122 :  58368.0\n",
      "123 :  61440.0\n",
      "124 :  62720.0\n",
      "125 :  65536.0\n",
      "126 :  58880.0\n",
      "127 :  61440.0\n",
      "128 :  62720.0\n",
      "129 :  60672.0\n",
      "130 :  59392.0\n",
      "131 :  58624.0\n",
      "132 :  59904.0\n",
      "133 :  61952.0\n",
      "134 :  68608.0\n",
      "135 :  62464.0\n",
      "136 :  60672.0\n",
      "137 :  57344.0\n",
      "138 :  57344.0\n",
      "139 :  67584.0\n",
      "140 :  63232.0\n",
      "141 :  56832.0\n",
      "142 :  60928.0\n",
      "143 :  59648.0\n",
      "144 :  65024.0\n",
      "145 :  62208.0\n",
      "146 :  68096.0\n",
      "147 :  66048.0\n",
      "148 :  61184.0\n",
      "149 :  64768.0\n",
      "150 :  57344.0\n",
      "151 :  58112.0\n",
      "152 :  60672.0\n",
      "153 :  64000.0\n",
      "154 :  61184.0\n",
      "155 :  60672.0\n",
      "156 :  60672.0\n",
      "157 :  64000.0\n",
      "158 :  64000.0\n",
      "159 :  58112.0\n",
      "160 :  60672.0\n",
      "161 :  61952.0\n",
      "162 :  60672.0\n",
      "163 :  59648.0\n",
      "164 :  57088.0\n",
      "165 :  59392.0\n",
      "166 :  55808.0\n",
      "167 :  60928.0\n",
      "168 :  59648.0\n",
      "169 :  64000.0\n",
      "170 :  57600.0\n",
      "171 :  62464.0\n",
      "172 :  65024.0\n",
      "173 :  58112.0\n",
      "174 :  65024.0\n",
      "175 :  66560.0\n",
      "176 :  57600.0\n",
      "177 :  64768.0\n",
      "178 :  62720.0\n",
      "179 :  61184.0\n",
      "180 :  66560.0\n",
      "181 :  65280.0\n",
      "182 :  67072.0\n",
      "183 :  58624.0\n",
      "184 :  66048.0\n",
      "185 :  61184.0\n",
      "186 :  57856.0\n",
      "187 :  52992.0\n",
      "188 :  59904.0\n",
      "189 :  66560.0\n",
      "190 :  59904.0\n",
      "191 :  57088.0\n",
      "192 :  59904.0\n",
      "193 :  64256.0\n",
      "194 :  56320.0\n",
      "195 :  67072.0\n",
      "196 :  63744.0\n",
      "197 :  60672.0\n",
      "198 :  57856.0\n",
      "199 :  67584.0\n",
      "200 :  63232.0\n",
      "201 :  60928.0\n",
      "202 :  57600.0\n",
      "203 :  67072.0\n",
      "204 :  57344.0\n",
      "205 :  59648.0\n",
      "206 :  60928.0\n",
      "207 :  63488.0\n",
      "208 :  55296.0\n",
      "209 :  56064.0\n",
      "210 :  63232.0\n",
      "211 :  57088.0\n",
      "212 :  66560.0\n",
      "213 :  61440.0\n",
      "214 :  64768.0\n",
      "215 :  57856.0\n",
      "216 :  59904.0\n",
      "217 :  56576.0\n",
      "218 :  60928.0\n",
      "219 :  58112.0\n",
      "220 :  64768.0\n",
      "221 :  58880.0\n",
      "222 :  65536.0\n",
      "223 :  59904.0\n",
      "224 :  61952.0\n",
      "225 :  64512.0\n",
      "226 :  56832.0\n",
      "227 :  62976.0\n",
      "228 :  64000.0\n",
      "229 :  59904.0\n",
      "230 :  59136.0\n",
      "231 :  63232.0\n",
      "232 :  64256.0\n",
      "233 :  65536.0\n",
      "234 :  57600.0\n",
      "235 :  71680.0\n",
      "236 :  58880.0\n",
      "237 :  60160.0\n",
      "238 :  58624.0\n",
      "239 :  58112.0\n",
      "240 :  60416.0\n",
      "241 :  62208.0\n",
      "242 :  66048.0\n",
      "243 :  68608.0\n",
      "244 :  57088.0\n",
      "245 :  62464.0\n",
      "246 :  61696.0\n",
      "247 :  56832.0\n",
      "248 :  58112.0\n",
      "249 :  58880.0\n",
      "250 :  57344.0\n",
      "251 :  60672.0\n",
      "252 :  57088.0\n",
      "253 :  59904.0\n",
      "254 :  62464.0\n",
      "255 :  57600.0\n",
      "256 :  60672.0\n",
      "257 :  61696.0\n",
      "258 :  64512.0\n",
      "259 :  66048.0\n",
      "260 :  61440.0\n",
      "261 :  64512.0\n",
      "262 :  60928.0\n",
      "263 :  60416.0\n",
      "264 :  60160.0\n",
      "265 :  58112.0\n",
      "266 :  60160.0\n",
      "267 :  62976.0\n",
      "268 :  62976.0\n",
      "269 :  65536.0\n",
      "270 :  66048.0\n",
      "271 :  64000.0\n",
      "272 :  60928.0\n",
      "273 :  58368.0\n",
      "274 :  59136.0\n",
      "275 :  61952.0\n",
      "276 :  57856.0\n",
      "277 :  54528.0\n",
      "278 :  57088.0\n",
      "279 :  60160.0\n",
      "280 :  55296.0\n",
      "281 :  64512.0\n",
      "282 :  64768.0\n",
      "283 :  58112.0\n",
      "284 :  60160.0\n",
      "285 :  58624.0\n",
      "286 :  57088.0\n",
      "287 :  59136.0\n",
      "288 :  65024.0\n",
      "289 :  57600.0\n",
      "290 :  59904.0\n",
      "291 :  62720.0\n",
      "292 :  55808.0\n",
      "293 :  66560.0\n",
      "294 :  62720.0\n",
      "295 :  58624.0\n",
      "296 :  59648.0\n",
      "297 :  58880.0\n",
      "298 :  58624.0\n",
      "299 :  53248.0\n",
      "300 :  62976.0\n",
      "301 :  62208.0\n",
      "302 :  58112.0\n",
      "303 :  56320.0\n",
      "304 :  68096.0\n",
      "305 :  56832.0\n",
      "306 :  60672.0\n",
      "307 :  61440.0\n",
      "308 :  60672.0\n",
      "309 :  61696.0\n",
      "310 :  61952.0\n",
      "311 :  61184.0\n",
      "312 :  62464.0\n",
      "313 :  59136.0\n",
      "314 :  61440.0\n",
      "315 :  52992.0\n",
      "316 :  62208.0\n",
      "317 :  60416.0\n",
      "318 :  63744.0\n",
      "319 :  55808.0\n",
      "320 :  63488.0\n",
      "321 :  61440.0\n",
      "322 :  56832.0\n",
      "323 :  64256.0\n",
      "324 :  61696.0\n",
      "325 :  60416.0\n",
      "326 :  69120.0\n",
      "327 :  62464.0\n",
      "328 :  57600.0\n",
      "329 :  63744.0\n",
      "330 :  56832.0\n",
      "331 :  59136.0\n",
      "332 :  55552.0\n",
      "333 :  66560.0\n",
      "334 :  70656.0\n",
      "335 :  62976.0\n",
      "336 :  55808.0\n",
      "337 :  65024.0\n",
      "338 :  55296.0\n",
      "339 :  59392.0\n",
      "340 :  55808.0\n",
      "341 :  59136.0\n",
      "342 :  58112.0\n",
      "343 :  60160.0\n",
      "344 :  61440.0\n",
      "345 :  62720.0\n",
      "346 :  63488.0\n",
      "347 :  66048.0\n",
      "348 :  58624.0\n",
      "349 :  62464.0\n",
      "350 :  63232.0\n",
      "351 :  66560.0\n",
      "352 :  59392.0\n",
      "353 :  57344.0\n",
      "354 :  60672.0\n",
      "355 :  60928.0\n",
      "356 :  60160.0\n",
      "357 :  61952.0\n",
      "358 :  60928.0\n",
      "359 :  62464.0\n",
      "360 :  65536.0\n",
      "361 :  58112.0\n",
      "362 :  58112.0\n",
      "363 :  55552.0\n",
      "364 :  62976.0\n",
      "365 :  64768.0\n",
      "366 :  60928.0\n",
      "367 :  61440.0\n",
      "368 :  57856.0\n",
      "369 :  59648.0\n",
      "370 :  62208.0\n",
      "371 :  54016.0\n",
      "372 :  71168.0\n",
      "373 :  68608.0\n",
      "374 :  64768.0\n",
      "375 :  63744.0\n",
      "376 :  56576.0\n",
      "377 :  58624.0\n",
      "378 :  59904.0\n",
      "379 :  62208.0\n",
      "380 :  58368.0\n",
      "381 :  59904.0\n",
      "382 :  59136.0\n",
      "383 :  62208.0\n",
      "384 :  65280.0\n",
      "385 :  59904.0\n",
      "386 :  65536.0\n",
      "387 :  59392.0\n",
      "388 :  53248.0\n",
      "389 :  66560.0\n",
      "390 :  60160.0\n",
      "391 :  55808.0\n",
      "392 :  64768.0\n",
      "393 :  60672.0\n",
      "394 :  70144.0\n",
      "395 :  62464.0\n",
      "396 :  64768.0\n",
      "397 :  57856.0\n",
      "398 :  57600.0\n",
      "399 :  60160.0\n",
      "400 :  58880.0\n",
      "401 :  60928.0\n",
      "402 :  62464.0\n",
      "403 :  61440.0\n",
      "404 :  59648.0\n",
      "405 :  64000.0\n",
      "406 :  58880.0\n",
      "407 :  64000.0\n",
      "408 :  62976.0\n",
      "409 :  63488.0\n",
      "410 :  64768.0\n",
      "411 :  53504.0\n",
      "412 :  66560.0\n",
      "413 :  56576.0\n",
      "414 :  56064.0\n",
      "415 :  62976.0\n",
      "416 :  59904.0\n",
      "417 :  61440.0\n",
      "418 :  56576.0\n",
      "419 :  60160.0\n",
      "420 :  62976.0\n",
      "421 :  57344.0\n",
      "422 :  56576.0\n",
      "423 :  55040.0\n",
      "424 :  63488.0\n",
      "425 :  57856.0\n",
      "426 :  66560.0\n",
      "427 :  69120.0\n",
      "428 :  62208.0\n",
      "429 :  63744.0\n",
      "430 :  64768.0\n",
      "431 :  60416.0\n",
      "432 :  61184.0\n",
      "433 :  64512.0\n",
      "434 :  58880.0\n",
      "435 :  63744.0\n",
      "436 :  62464.0\n",
      "437 :  61696.0\n",
      "438 :  65280.0\n",
      "439 :  64256.0\n",
      "440 :  59648.0\n",
      "441 :  63488.0\n",
      "442 :  54784.0\n",
      "443 :  58624.0\n",
      "444 :  64256.0\n",
      "445 :  59392.0\n",
      "446 :  60672.0\n",
      "447 :  62976.0\n",
      "448 :  61440.0\n",
      "449 :  59648.0\n",
      "450 :  56832.0\n",
      "451 :  61952.0\n",
      "452 :  61696.0\n",
      "453 :  56832.0\n",
      "454 :  56832.0\n",
      "455 :  57344.0\n",
      "456 :  59136.0\n",
      "457 :  59648.0\n",
      "458 :  59648.0\n",
      "459 :  57344.0\n",
      "460 :  60416.0\n",
      "461 :  58112.0\n",
      "462 :  62976.0\n",
      "463 :  57344.0\n",
      "464 :  59648.0\n",
      "465 :  61440.0\n",
      "466 :  73216.0\n",
      "467 :  61184.0\n",
      "468 :  62720.0\n",
      "469 :  60928.0\n",
      "470 :  62208.0\n",
      "471 :  62208.0\n",
      "472 :  60160.0\n",
      "473 :  58368.0\n",
      "474 :  61952.0\n",
      "475 :  61184.0\n",
      "476 :  61952.0\n",
      "477 :  58624.0\n",
      "478 :  60160.0\n",
      "479 :  66048.0\n",
      "480 :  61440.0\n",
      "481 :  59392.0\n",
      "482 :  56320.0\n",
      "483 :  61184.0\n",
      "484 :  67584.0\n",
      "485 :  65536.0\n",
      "486 :  66048.0\n",
      "487 :  60672.0\n",
      "488 :  59392.0\n",
      "489 :  62208.0\n",
      "490 :  58624.0\n",
      "491 :  65024.0\n",
      "492 :  63488.0\n",
      "493 :  58624.0\n",
      "494 :  65024.0\n",
      "495 :  57088.0\n",
      "496 :  54272.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m train_history[tot_step] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m \u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(f\"Epoch {step_i}, Learning Rate: {scheduler.get_last_lr()}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/optim/optimizer.py:87\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m prev_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Note on graph break below:\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# we need to graph break to ensure that aot respects the no_grad annotation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# see https://github.com/pytorch/pytorch/issues/104053\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m     89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/AI_ENV/lib/python3.10/site-packages/torch/autograd/grad_mode.py:187\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 187\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "# train_history = {}\n",
    "# val_history = {}\n",
    "for step_i in range(num_steps):\n",
    "    tot_step = step_i + start_step\n",
    "    optimiser.zero_grad()\n",
    "    x_batch, y_batch = get_batch(X, y, bs) \n",
    "    # print(x_batch, y_batch)\n",
    "    y_pred = model(x_batch).view(bs)\n",
    "    # print(y_pred.shape, y_batch.shape)\n",
    "    loss = loss_category(y_pred, y_batch)\n",
    "    # print(loss.item())\n",
    "    train_history[tot_step] = loss.item()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    scheduler.step()\n",
    "    # print(f\"Epoch {step_i}, Learning Rate: {scheduler.get_last_lr()}\")\n",
    "    print(tot_step, ': ',loss.item())\n",
    "\n",
    "    if tot_step % 100 == 0:\n",
    "        # validation phase\n",
    "        y_pred = model(X_val).view(val_split)\n",
    "        # print(y_pred.shape, y_batch.shape)\n",
    "        loss = loss_category(y_pred, y_val)\n",
    "        # print(loss.item())\n",
    "        val_history[tot_step] = loss.item()\n",
    "\n",
    "start_step += num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saves/bad_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(train_history.keys())[500:], list(train_history.values())[500:], label='train')\n",
    "# plt.plot(val_history.keys(), val_history.values(), label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(train_history.keys()), list(train_history.values()), label='train')\n",
    "plt.plot(val_history.keys(), val_history.values(), label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# My results\n",
    "At relu, model is stuck around 25k with 3x1000 steps with xavier\n",
    "AT relu, moedl went to 9k and then exploded at 3x1000 steps with kaiming; then at 14k\n",
    "At gelu, model is stuck atound 10k\n",
    "\n",
    "Adam is better than AdamW for this task\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] do inference, and run a partially trained model with the GUI intact\n",
    "- [ ] **find a way to fix the fact that our model is giving integer loss\n",
    "- [ ] **Fix the bug in initialisation\n",
    "- [ ] implement weights and biases or tensorboard \n",
    "- [ ] improve the model\n",
    "  - [ ] get a better/ bigger dataset\n",
    "  - [ ] hyperparameter and architecture\n",
    "    - [ ] add CNN\n",
    "    - [x] better encoding\n",
    "    - [ ] residual connections\n",
    "    - [ ] try adamW after tuning b1 and b2\n",
    "    - [ ] increase embedding dim\n",
    "    - [ ] increase neurons in the layers\n",
    "    - [ ] increase layers in the network\n",
    "    - [ ] change loss function (maybe)\n",
    "    - [ ] try diff learning rate scheduler(trapeziodal)\n",
    "    - [ ] Add regularisation\n",
    "      - [ ] l1,l2\n",
    "      - [ ] dropout\n",
    "    - [ ] Better initialisation\n",
    "    - [ ] diff optimisation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make the init proper by specifying the activation in the init as claude said\n",
    "-  no cnn right now, maybe in future\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
